# #83 Machine Learning & Data Science Challenge 83

# What do you understand by tokenization?

**Tokenization is the act of breaking a sequence of strings into pieces such as words, keywords, phrases, symbols, and other elements called tokens.**

* Tokens can be individual words, phrases, or even whole sentences. In the process of tokenization, some characters like punctuation marks are discarded.
    

> ![](https://cdn.hashnode.com/res/hashnode/image/upload/v1675181013232/4ae64e48-32cc-4675-90f9-bccd739ab2b9.png align="center")